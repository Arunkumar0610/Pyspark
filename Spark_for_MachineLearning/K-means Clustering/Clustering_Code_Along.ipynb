{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3TfpgZnTYzx",
        "outputId": "e52e80ff-89c5-448e-a782-97bc0d910fb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=ee983dae9ea7212823af655a3f5e4a69b5b39f0118f62eea30eac5fad6f8d665\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark py4j"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clustering Code Along\n",
        "\n",
        "We'll be working with a real data set about seeds, from UCI repository: https://archive.ics.uci.edu/ml/datasets/seeds.\n",
        "\n",
        "The examined group comprised kernels belonging to three different varieties of wheat: Kama, Rosa and Canadian, 70 elements each, randomly selected for the experiment. High quality visualization of the internal kernel structure was detected using a soft X-ray technique. It is non-destructive and considerably cheaper than other more sophisticated imaging techniques like scanning microscopy or laser technology. The images were recorded on 13x18 cm X-ray KODAK plates. Studies were conducted using combine harvested wheat grain originating from experimental fields, explored at the Institute of Agrophysics of the Polish Academy of Sciences in Lublin.\n",
        "\n",
        "The data set can be used for the tasks of classification and cluster analysis.\n",
        "\n",
        "Attribute Information:\n",
        "\n",
        "To construct the data, seven geometric parameters of wheat kernels were measured:\n",
        "\n",
        "1. area A,\n",
        "2. perimeter P,\n",
        "3. compactness C = 4piA/P^2,\n",
        "4. length of kernel,\n",
        "5. width of kernel,\n",
        "6. asymmetry coefficient\n",
        "7. length of kernel groove.\n",
        "\n",
        "All of these parameters were real-valued continuous.\n",
        "\n",
        "Let's see if we can cluster them in to 3 groups with K-means!"
      ],
      "metadata": {
        "id": "m9w05tXYTkWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('cluster').getOrCreate()"
      ],
      "metadata": {
        "id": "Ohj4boH1T9Yy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads data.\n",
        "dataset = spark.read.csv(\"/content/sample_data/seeds_dataset.csv\",header=True,inferSchema=True)"
      ],
      "metadata": {
        "id": "_QmSr8mOUTMj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9oU8CmkUtsF",
        "outputId": "0aea2040-c985-471c-9f0d-adf42c6ff0d0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- area: double (nullable = true)\n",
            " |-- perimeter: double (nullable = true)\n",
            " |-- compactness: double (nullable = true)\n",
            " |-- length_of_kernel: double (nullable = true)\n",
            " |-- width_of_kernel: double (nullable = true)\n",
            " |-- asymmetry_coefficient: double (nullable = true)\n",
            " |-- length_of_groove: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xdyw1chUzb_",
        "outputId": "44884ba7-4441-4bb8-813f-3d927b14cfe4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(area=15.26, perimeter=14.84, compactness=0.871, length_of_kernel=5.763, width_of_kernel=3.312, asymmetry_coefficient=2.221, length_of_groove=5.22)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Format the Data"
      ],
      "metadata": {
        "id": "YIGDlQgzXqtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.feature import VectorAssembler"
      ],
      "metadata": {
        "id": "Jepf9uJYVEDe"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lqzUlO9VRaI",
        "outputId": "1d5e28ac-66c1-4448-aaf5-7eb650d8dd8b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['area',\n",
              " 'perimeter',\n",
              " 'compactness',\n",
              " 'length_of_kernel',\n",
              " 'width_of_kernel',\n",
              " 'asymmetry_coefficient',\n",
              " 'length_of_groove']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assembler=VectorAssembler(inputCols=dataset.columns,outputCol='features')"
      ],
      "metadata": {
        "id": "HjR093_tVVXg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_data=assembler.transform(dataset)"
      ],
      "metadata": {
        "id": "mesXXWGrVhfP"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_data.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUScqszfVpK5",
        "outputId": "c3601505-81fd-464f-c252-3cea919e27c2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- area: double (nullable = true)\n",
            " |-- perimeter: double (nullable = true)\n",
            " |-- compactness: double (nullable = true)\n",
            " |-- length_of_kernel: double (nullable = true)\n",
            " |-- width_of_kernel: double (nullable = true)\n",
            " |-- asymmetry_coefficient: double (nullable = true)\n",
            " |-- length_of_groove: double (nullable = true)\n",
            " |-- features: vector (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scale the Data\n",
        "\n",
        "It is a good idea to scale our data to deal with the curse of dimensionality: https://en.wikipedia.org/wiki/Curse_of_dimensionality"
      ],
      "metadata": {
        "id": "8FEV7kcBXuQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StandardScaler"
      ],
      "metadata": {
        "id": "LZMevFOgVszc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler=StandardScaler(inputCol='features',outputCol='scaledFeatures',withStd=True, withMean=False)"
      ],
      "metadata": {
        "id": "FADm4-zAVy_h"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute summary statistics by fitting the StandardScaler\n",
        "scaler_model=scaler.fit(final_data)"
      ],
      "metadata": {
        "id": "UEWzoG94WBWe"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize each feature to have unit standard deviation.\n",
        "final_data=scaler_model.transform(final_data)"
      ],
      "metadata": {
        "id": "qNzpqj6XWGhS"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_data.head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUy5kRusWPHf",
        "outputId": "f8636d4d-5766-4801-aed5-cdecc1ce2efc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(area=15.26, perimeter=14.84, compactness=0.871, length_of_kernel=5.763, width_of_kernel=3.312, asymmetry_coefficient=2.221, length_of_groove=5.22, features=DenseVector([15.26, 14.84, 0.871, 5.763, 3.312, 2.221, 5.22]), scaledFeatures=DenseVector([5.2445, 11.3633, 36.8608, 13.0072, 8.7685, 1.4772, 10.621]))]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Model and Evaluate"
      ],
      "metadata": {
        "id": "ig7v_xEKX6VG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trains a k-means model.\n",
        "kmeans=KMeans(featuresCol='scaledFeatures',k=3)"
      ],
      "metadata": {
        "id": "w_oGCXcvWUTD"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=kmeans.fit(final_data)"
      ],
      "metadata": {
        "id": "inDEwya0WdHX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate clustering by computing Within Set Sum of Squared Errors.\n",
        "print(\"WSSSE: {}\".format(model.summary.trainingCost))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ2Ul5SkWhju",
        "outputId": "d0714049-38c2-4b94-8bff-9d1a944a1c8c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WSSSE: 429.07559671507244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shows the result.\n",
        "centers=model.clusterCenters()\n",
        "print(\"Cluster Centers :\")\n",
        "for i in centers:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qp1YcVw0W0zB",
        "outputId": "fde93c04-aee3-4e78-8835-64f9402dd6d5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster Centers :\n",
            "[ 4.87257659 10.88120146 37.27692543 12.3410157   8.55443412  1.81649011\n",
            " 10.32998598]\n",
            "[ 6.31670546 12.37109759 37.39491396 13.91155062  9.748067    2.39849968\n",
            " 12.2661748 ]\n",
            "[ 4.06105916 10.13979506 35.80536984 11.82133095  7.50395937  3.27184732\n",
            " 10.42126018]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.transform(final_data).select('prediction').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-KOwIElXH8N",
        "outputId": "4748a59c-6d9c-4447-92a2-fab47b554624"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|prediction|\n",
            "+----------+\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         1|\n",
            "|         1|\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         2|\n",
            "+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}